{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwJ7igw4mfgt",
    "outputId": "57f0f82e-157e-4ae0-e5cf-bbc734f1655f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.4.0\n",
      "Hub version:  0.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from tensorflow.keras.models import  Model\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from sklearn import preprocessing\n",
    "from bert import bert_tokenization\n",
    "print(\"TensorFlow Version:\",tf.__version__)\n",
    "print(\"Hub version: \",hub.__version__)\n",
    "\n",
    "# Install following packages before going forward\n",
    "# pip install bert-for-tf2\n",
    "# pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1QixDBKKoESF"
   },
   "outputs": [],
   "source": [
    "# Load train and val datasets\n",
    "df_train = pd.read_csv('tamil_train.tsv', sep = \"\\t\")\n",
    "df_val = pd.read_csv('tamil_dev.tsv', sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YEXVtLY4zKXA",
    "outputId": "865fe230-df84-493d-a94a-fddf880498bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'category'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BW-PO8F8p4HA",
    "outputId": "3e2aa8da-76e8-45a2-f39a-915b387d4b73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique labels 5\n"
     ]
    }
   ],
   "source": [
    "# Prepare input text and one hot encoded labels for train and validation sets\n",
    "\n",
    "unique_labels = list(np.unique(df_train[\"category\"]))\n",
    "\n",
    "train_x = df_train[\"text\"].values\n",
    "train_y = df_train[\"category\"].values\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "train_y = le.fit_transform(train_y)\n",
    "train_y = tf.keras.utils.to_categorical(train_y, num_classes=len(unique_labels), dtype='float32')\n",
    "\n",
    "val_x = df_val[\"text\"].values\n",
    "val_y = df_val[\"category\"].values\n",
    "\n",
    "val_y = le.fit_transform(val_y)\n",
    "val_y = tf.keras.utils.to_categorical(val_y, num_classes=len(unique_labels), dtype='float32')\n",
    "\n",
    "\n",
    "print(\"number of unique labels\", len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nUD20x5Oqzgp",
    "outputId": "0c46afe8-05df-4ba5-d3f4-44fd59ef3608"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mixed_feelings ', 'Negative ', 'Positive ', 'not-Tamil ', 'unknown_state ']"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check unique labels\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nEHX4WgmnIkM"
   },
   "outputs": [],
   "source": [
    "# Function to create input_ids\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "# Function to create attention masks\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "# Function to create segment ids\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "# Function to create input_ids, attention_masks, segment_ids for sample\n",
    "def create_single_input(sentence,MAX_LEN, MAX_SEQ_LEN):\n",
    "  \n",
    "  stokens = tokenizer.tokenize(sentence)\n",
    "  \n",
    "  stokens = stokens[:MAX_LEN]\n",
    "  \n",
    "  stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    " \n",
    "  ids = get_ids(stokens, tokenizer, MAX_SEQ_LEN)\n",
    "  masks = get_masks(stokens, MAX_SEQ_LEN)\n",
    "  segments = get_segments(stokens, MAX_SEQ_LEN)\n",
    "\n",
    "  return ids,masks,segments\n",
    "\n",
    "def create_input_array(sentences, MAX_SEQ_LEN):\n",
    "\n",
    "  input_ids, input_masks, input_segments = [], [], []\n",
    "\n",
    "  for sentence in tqdm(sentences,position=0, leave=True):\n",
    "  \n",
    "    ids,masks,segments=create_single_input(sentence,MAX_SEQ_LEN-2, MAX_SEQ_LEN)\n",
    "\n",
    "    input_ids.append(ids)\n",
    "    input_masks.append(masks)\n",
    "    input_segments.append(segments)\n",
    "\n",
    "  return [np.asarray(input_ids, dtype=np.int32), \n",
    "            np.asarray(input_masks, dtype=np.int32), \n",
    "            np.asarray(input_segments, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Q8AmiWuknTKL"
   },
   "outputs": [],
   "source": [
    "# MuRIL model layer\n",
    "muril_layer = hub.KerasLayer(\"https://tfhub.dev/google/MuRIL/1\", trainable=True)\n",
    "\n",
    "# Create tokenizer\n",
    "vocab_file = muril_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = muril_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDqjXUqturNk",
    "outputId": "289dd609-7759-4f52-f943-981516eb66ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11335/11335 [00:01<00:00, 6271.16it/s]\n",
      "100%|██████████| 1260/1260 [00:00<00:00, 5982.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create input_ids, attention_masks, segment_ids for training and validation sets with max_seq_len as 128\n",
    "max_seq_len = 128\n",
    "train_x = create_input_array(train_x, max_seq_len)\n",
    "val_x = create_input_array(val_x, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DtImk-2eqn4a"
   },
   "outputs": [],
   "source": [
    "# Define model function - compile and fit\n",
    "def model_fit(train_x, train_y, val_x, val_y, max_seq_length, num_epochs, muril_layer):\n",
    "\n",
    "  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "  segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "  \n",
    "  outputs = muril_layer(dict(input_word_ids = input_word_ids, input_mask = input_mask, input_type_ids = segment_ids))\n",
    "\n",
    "  x = tf.keras.layers.Dropout(0.2)(outputs[\"pooled_output\"]) # take pooled output layer\n",
    "  final_output = tf.keras.layers.Dense(5, activation=\"softmax\", name=\"dense_output\")(x)\n",
    "\n",
    "  model = tf.keras.models.Model(\n",
    "      inputs=[input_word_ids, input_mask, segment_ids], outputs=final_output)\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "  model.fit(train_x, train_y, epochs = num_epochs, batch_size = 32, validation_data = (val_x, val_y), shuffle = True)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfQ_Jv6sulcq",
    "outputId": "46a72151-7ba2-4bb5-d224-9052eb8ffab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/355 [==============================] - 350s 954ms/step - loss: 1.0874 - accuracy: 0.6662 - val_loss: 1.0215 - val_accuracy: 0.6802\n"
     ]
    }
   ],
   "source": [
    "# Set number of epochs\n",
    "num_epochs = 1\n",
    "\n",
    "# Get the model object\n",
    "model = model_fit(train_x, train_y, val_x, val_y, max_seq_len, num_epochs, muril_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "E8Y7dCzfrDJT"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "preds = model.predict(val_x)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MuRIL_finetuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
